{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2Ul6RFlUpLw"
      },
      "source": [
        "# Fine-tuning with Dreambooth\n",
        "\n",
        "Made to use on colab's GPU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDOGCtcMdVz8",
        "outputId": "1d759321-b7ba-47b5-f284-df567ae1dc22"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BuywrgIS_fa",
        "outputId": "b17386bc-6ac6-4a2a-c29b-fe1628a0c340"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAjFcWXwUpLz",
        "outputId": "a0d23e98-13f6-407a-f6a4-c44249d57993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'diffusers'...\n",
            "remote: Enumerating objects: 56940, done.\u001b[K\n",
            "remote: Counting objects: 100% (1120/1120), done.\u001b[K\n",
            "remote: Compressing objects: 100% (581/581), done.\u001b[K\n",
            "remote: Total 56940 (delta 691), reused 749 (delta 433), pack-reused 55820\u001b[K\n",
            "Receiving objects: 100% (56940/56940), 39.29 MiB | 14.48 MiB/s, done.\n",
            "Resolving deltas: 100% (40936/40936), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/diffusers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd diffusers\n",
        "!pip install .\n",
        "%cd examples/dreambooth\n",
        "!pip install -r requirements.txt\n",
        "!accelerate config default"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnUsrnUSVB1N",
        "outputId": "97084e4c-8e75-43e4-f9d2-ac06a8a681d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/diffusers\n",
            "Processing /content/diffusers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.27.0.dev0) (7.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.27.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.2 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.27.0.dev0) (0.20.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers==0.27.0.dev0) (1.25.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.27.0.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.27.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.27.0.dev0) (0.4.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.27.0.dev0) (9.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers==0.27.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers==0.27.0.dev0) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers==0.27.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers==0.27.0.dev0) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers==0.27.0.dev0) (23.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.27.0.dev0) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.27.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.27.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.27.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.27.0.dev0) (2024.2.2)\n",
            "Building wheels for collected packages: diffusers\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.27.0.dev0-py3-none-any.whl size=1982931 sha256=d62418d21b1887ac8c48a30bd38e096c3f0a08dbcefbbffb566f10f030492c98\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o0anm0on/wheels/95/c5/3b/e1b4269f8a2584de57e75f949a185b48fc4144e9a91fc9965a\n",
            "Successfully built diffusers\n",
            "Installing collected packages: diffusers\n",
            "Successfully installed diffusers-0.27.0.dev0\n",
            "/content/diffusers/examples/dreambooth\n",
            "Collecting accelerate>=0.16.0 (from -r requirements.txt (line 1))\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.16.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.38.2)\n",
            "Collecting ftfy (from -r requirements.txt (line 4))\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2.15.2)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.1.3)\n",
            "Collecting peft==0.7.0 (from -r requirements.txt (line 7))\n",
            "  Downloading peft-0.7.0-py3-none-any.whl (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r requirements.txt (line 7)) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r requirements.txt (line 7)) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r requirements.txt (line 7)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r requirements.txt (line 7)) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r requirements.txt (line 7)) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r requirements.txt (line 7)) (4.66.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r requirements.txt (line 7)) (0.4.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.0->-r requirements.txt (line 7)) (0.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 3)) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 3)) (0.15.2)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->-r requirements.txt (line 4)) (0.2.13)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.62.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (3.5.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 5)) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2->-r requirements.txt (line 6)) (2.1.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2024.2.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.7.0->-r requirements.txt (line 7)) (1.3.0)\n",
            "Installing collected packages: ftfy, accelerate, peft\n",
            "Successfully installed accelerate-0.27.2 ftfy-6.1.3 peft-0.7.0\n",
            "accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfHIjJpJUpL2"
      },
      "outputs": [],
      "source": [
        "# Links to go from dreambooth to drive from here to save the results\n",
        "db_to_drive_path = './../../../drive/MyDrive/SketchToReality/'\n",
        "drive_to_db_path = './../../../../diffusers/examples/dreambooth' # In case, to come back :)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4JyBfBjWkaT",
        "outputId": "6d7850b6-37ab-48c2-e70a-c9b55d480029"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.md              requirements.txt             train_dreambooth_flax.py\n",
            "README_sdxl.md         test_dreambooth_lora_edm.py  train_dreambooth_lora.py\n",
            "requirements_flax.txt  test_dreambooth_lora.py      train_dreambooth_lora_sdxl.py\n",
            "requirements_sdxl.txt  test_dreambooth.py           train_dreambooth.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__________________________\n",
        "**VERY IMPORTANT**\n",
        "\n",
        "1. Colab doesn't support the standard commands on regular terminals.\n",
        "To avoid errors when running dreambooth with the command line in the huggingface tutorial, change `train_dreambooth.py`at the 3rd line before the end:\n",
        "\n",
        "- Delete the `if __name__ == 'main':`\n",
        "- Unindent the lines below\n",
        "At the end of the file, you should have\n",
        "```\n",
        "args=parseargs()\n",
        "main(args)\n",
        "```\n",
        "2. Don't forget to go on the 8Gb GPU version on the huggingface tutorial, as 16Gb won't be supported on colab.\n",
        "\n",
        "3. Make sure your data and results paths are valid and that the folders do not already contain data\n",
        "\n",
        "______________________________"
      ],
      "metadata": {
        "id": "THdyDE9tC5UK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q diffusers transformers accelerate peft # Trick found online to make it work\n",
        "!pip install xformers\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJjTRRlmEq9s",
        "outputId": "c8d97c4d-be36-452a-b061-0202cca961d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xformers\n",
            "  Downloading xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (218.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.2/218.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.25.2)\n",
            "Collecting torch==2.2.0 (from xformers)\n",
            "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0->xformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0->xformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0->xformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0->xformers)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0->xformers)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0->xformers)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0->xformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0->xformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0->xformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0->xformers)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0->xformers)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2.0 (from torch==2.2.0->xformers)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->xformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0->xformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0->xformers) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 torch-2.2.0 triton-2.2.0 xformers-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show possible parameters to pass\n",
        "!accelerate launch train_dreambooth.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H6QAqTCJUMs",
        "outputId": "50eb7a58-426e-4743-e427-2518c4a136b7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "2024-03-08 22:18:47.540100: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 22:18:47.540182: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 22:18:47.542260: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 22:18:49.525034: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "usage: train_dreambooth.py [-h] --pretrained_model_name_or_path PRETRAINED_MODEL_NAME_OR_PATH\n",
            "                           [--revision REVISION] [--variant VARIANT]\n",
            "                           [--tokenizer_name TOKENIZER_NAME] --instance_data_dir INSTANCE_DATA_DIR\n",
            "                           [--class_data_dir CLASS_DATA_DIR] --instance_prompt INSTANCE_PROMPT\n",
            "                           [--class_prompt CLASS_PROMPT] [--with_prior_preservation]\n",
            "                           [--prior_loss_weight PRIOR_LOSS_WEIGHT]\n",
            "                           [--num_class_images NUM_CLASS_IMAGES] [--output_dir OUTPUT_DIR]\n",
            "                           [--seed SEED] [--resolution RESOLUTION] [--center_crop]\n",
            "                           [--train_text_encoder] [--train_batch_size TRAIN_BATCH_SIZE]\n",
            "                           [--sample_batch_size SAMPLE_BATCH_SIZE]\n",
            "                           [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                           [--max_train_steps MAX_TRAIN_STEPS]\n",
            "                           [--checkpointing_steps CHECKPOINTING_STEPS]\n",
            "                           [--checkpoints_total_limit CHECKPOINTS_TOTAL_LIMIT]\n",
            "                           [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "                           [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                           [--gradient_checkpointing] [--learning_rate LEARNING_RATE] [--scale_lr]\n",
            "                           [--lr_scheduler LR_SCHEDULER] [--lr_warmup_steps LR_WARMUP_STEPS]\n",
            "                           [--lr_num_cycles LR_NUM_CYCLES] [--lr_power LR_POWER] [--use_8bit_adam]\n",
            "                           [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
            "                           [--adam_beta1 ADAM_BETA1] [--adam_beta2 ADAM_BETA2]\n",
            "                           [--adam_weight_decay ADAM_WEIGHT_DECAY] [--adam_epsilon ADAM_EPSILON]\n",
            "                           [--max_grad_norm MAX_GRAD_NORM] [--push_to_hub] [--hub_token HUB_TOKEN]\n",
            "                           [--hub_model_id HUB_MODEL_ID] [--logging_dir LOGGING_DIR]\n",
            "                           [--allow_tf32] [--report_to REPORT_TO]\n",
            "                           [--validation_prompt VALIDATION_PROMPT]\n",
            "                           [--num_validation_images NUM_VALIDATION_IMAGES]\n",
            "                           [--validation_steps VALIDATION_STEPS]\n",
            "                           [--mixed_precision {no,fp16,bf16}]\n",
            "                           [--prior_generation_precision {no,fp32,fp16,bf16}]\n",
            "                           [--local_rank LOCAL_RANK]\n",
            "                           [--enable_xformers_memory_efficient_attention] [--set_grads_to_none]\n",
            "                           [--offset_noise] [--snr_gamma SNR_GAMMA]\n",
            "                           [--pre_compute_text_embeddings]\n",
            "                           [--tokenizer_max_length TOKENIZER_MAX_LENGTH]\n",
            "                           [--text_encoder_use_attention_mask] [--skip_save_text_encoder]\n",
            "                           [--validation_images VALIDATION_IMAGES [VALIDATION_IMAGES ...]]\n",
            "                           [--class_labels_conditioning CLASS_LABELS_CONDITIONING]\n",
            "                           [--validation_scheduler {DPMSolverMultistepScheduler,DDPMScheduler}]\n",
            "\n",
            "Simple example of a training script.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --pretrained_model_name_or_path PRETRAINED_MODEL_NAME_OR_PATH\n",
            "                        Path to pretrained model or model identifier from huggingface.co/models.\n",
            "  --revision REVISION   Revision of pretrained model identifier from huggingface.co/models.\n",
            "  --variant VARIANT     Variant of the model files of the pretrained model identifier from\n",
            "                        huggingface.co/models, 'e.g.' fp16\n",
            "  --tokenizer_name TOKENIZER_NAME\n",
            "                        Pretrained tokenizer name or path if not the same as model_name\n",
            "  --instance_data_dir INSTANCE_DATA_DIR\n",
            "                        A folder containing the training data of instance images.\n",
            "  --class_data_dir CLASS_DATA_DIR\n",
            "                        A folder containing the training data of class images.\n",
            "  --instance_prompt INSTANCE_PROMPT\n",
            "                        The prompt with identifier specifying the instance\n",
            "  --class_prompt CLASS_PROMPT\n",
            "                        The prompt to specify images in the same class as provided instance\n",
            "                        images.\n",
            "  --with_prior_preservation\n",
            "                        Flag to add prior preservation loss.\n",
            "  --prior_loss_weight PRIOR_LOSS_WEIGHT\n",
            "                        The weight of prior preservation loss.\n",
            "  --num_class_images NUM_CLASS_IMAGES\n",
            "                        Minimal class images for prior preservation loss. If there are not enough\n",
            "                        images already present in class_data_dir, additional images will be\n",
            "                        sampled with class_prompt.\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        The output directory where the model predictions and checkpoints will be\n",
            "                        written.\n",
            "  --seed SEED           A seed for reproducible training.\n",
            "  --resolution RESOLUTION\n",
            "                        The resolution for input images, all the images in the train/validation\n",
            "                        dataset will be resized to this resolution\n",
            "  --center_crop         Whether to center crop the input images to the resolution. If not set, the\n",
            "                        images will be randomly cropped. The images will be resized to the\n",
            "                        resolution first before cropping.\n",
            "  --train_text_encoder  Whether to train the text encoder. If set, the text encoder should be\n",
            "                        float32 precision.\n",
            "  --train_batch_size TRAIN_BATCH_SIZE\n",
            "                        Batch size (per device) for the training dataloader.\n",
            "  --sample_batch_size SAMPLE_BATCH_SIZE\n",
            "                        Batch size (per device) for sampling images.\n",
            "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
            "  --max_train_steps MAX_TRAIN_STEPS\n",
            "                        Total number of training steps to perform. If provided, overrides\n",
            "                        num_train_epochs.\n",
            "  --checkpointing_steps CHECKPOINTING_STEPS\n",
            "                        Save a checkpoint of the training state every X updates. Checkpoints can\n",
            "                        be used for resuming training via `--resume_from_checkpoint`. In the case\n",
            "                        that the checkpoint is better than the final trained model, the checkpoint\n",
            "                        can also be used for inference.Using a checkpoint for inference requires\n",
            "                        separate loading of the original pipeline and the individual checkpointed\n",
            "                        model components.See https://huggingface.co/docs/diffusers/main/en/trainin\n",
            "                        g/dreambooth#performing-inference-using-a-saved-checkpoint for step by\n",
            "                        stepinstructions.\n",
            "  --checkpoints_total_limit CHECKPOINTS_TOTAL_LIMIT\n",
            "                        Max number of checkpoints to store. Passed as `total_limit` to the\n",
            "                        `Accelerator` `ProjectConfiguration`. See Accelerator::save_state https://\n",
            "                        huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Ac\n",
            "                        celerator.save_state for more details\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        Whether training should be resumed from a previous checkpoint. Use a path\n",
            "                        saved by `--checkpointing_steps`, or `\"latest\"` to automatically select\n",
            "                        the last available checkpoint.\n",
            "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
            "                        Number of updates steps to accumulate before performing a backward/update\n",
            "                        pass.\n",
            "  --gradient_checkpointing\n",
            "                        Whether or not to use gradient checkpointing to save memory at the expense\n",
            "                        of slower backward pass.\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        Initial learning rate (after the potential warmup period) to use.\n",
            "  --scale_lr            Scale the learning rate by the number of GPUs, gradient accumulation\n",
            "                        steps, and batch size.\n",
            "  --lr_scheduler LR_SCHEDULER\n",
            "                        The scheduler type to use. Choose between [\"linear\", \"cosine\",\n",
            "                        \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n",
            "  --lr_warmup_steps LR_WARMUP_STEPS\n",
            "                        Number of steps for the warmup in the lr scheduler.\n",
            "  --lr_num_cycles LR_NUM_CYCLES\n",
            "                        Number of hard resets of the lr in cosine_with_restarts scheduler.\n",
            "  --lr_power LR_POWER   Power factor of the polynomial scheduler.\n",
            "  --use_8bit_adam       Whether or not to use 8-bit Adam from bitsandbytes.\n",
            "  --dataloader_num_workers DATALOADER_NUM_WORKERS\n",
            "                        Number of subprocesses to use for data loading. 0 means that the data will\n",
            "                        be loaded in the main process.\n",
            "  --adam_beta1 ADAM_BETA1\n",
            "                        The beta1 parameter for the Adam optimizer.\n",
            "  --adam_beta2 ADAM_BETA2\n",
            "                        The beta2 parameter for the Adam optimizer.\n",
            "  --adam_weight_decay ADAM_WEIGHT_DECAY\n",
            "                        Weight decay to use.\n",
            "  --adam_epsilon ADAM_EPSILON\n",
            "                        Epsilon value for the Adam optimizer\n",
            "  --max_grad_norm MAX_GRAD_NORM\n",
            "                        Max gradient norm.\n",
            "  --push_to_hub         Whether or not to push the model to the Hub.\n",
            "  --hub_token HUB_TOKEN\n",
            "                        The token to use to push to the Model Hub.\n",
            "  --hub_model_id HUB_MODEL_ID\n",
            "                        The name of the repository to keep in sync with the local `output_dir`.\n",
            "  --logging_dir LOGGING_DIR\n",
            "                        [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will\n",
            "                        default to *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\n",
            "  --allow_tf32          Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up\n",
            "                        training. For more information, see\n",
            "                        https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-\n",
            "                        ampere-devices\n",
            "  --report_to REPORT_TO\n",
            "                        The integration to report the results and logs to. Supported platforms are\n",
            "                        `\"tensorboard\"` (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to\n",
            "                        report to all integrations.\n",
            "  --validation_prompt VALIDATION_PROMPT\n",
            "                        A prompt that is used during validation to verify that the model is\n",
            "                        learning.\n",
            "  --num_validation_images NUM_VALIDATION_IMAGES\n",
            "                        Number of images that should be generated during validation with\n",
            "                        `validation_prompt`.\n",
            "  --validation_steps VALIDATION_STEPS\n",
            "                        Run validation every X steps. Validation consists of running the prompt\n",
            "                        `args.validation_prompt` multiple times: `args.num_validation_images` and\n",
            "                        logging the images.\n",
            "  --mixed_precision {no,fp16,bf16}\n",
            "                        Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16).\n",
            "                        Bf16 requires PyTorch >= 1.10.and an Nvidia Ampere GPU. Default to the\n",
            "                        value of accelerate config of the current system or the flag passed with\n",
            "                        the `accelerate.launch` command. Use this argument to override the\n",
            "                        accelerate config.\n",
            "  --prior_generation_precision {no,fp32,fp16,bf16}\n",
            "                        Choose prior generation precision between fp32, fp16 and bf16 (bfloat16).\n",
            "                        Bf16 requires PyTorch >= 1.10.and an Nvidia Ampere GPU. Default to fp16 if\n",
            "                        a GPU is available else fp32.\n",
            "  --local_rank LOCAL_RANK\n",
            "                        For distributed training: local_rank\n",
            "  --enable_xformers_memory_efficient_attention\n",
            "                        Whether or not to use xformers.\n",
            "  --set_grads_to_none   Save more memory by using setting grads to None instead of zero. Be aware,\n",
            "                        that this changes certain behaviors, so disable this argument if it causes\n",
            "                        any problems. More info: https://pytorch.org/docs/stable/generated/torch.o\n",
            "                        ptim.Optimizer.zero_grad.html\n",
            "  --offset_noise        Fine-tuning against a modified noise See:\n",
            "                        https://www.crosslabs.org//blog/diffusion-with-offset-noise for more\n",
            "                        information.\n",
            "  --snr_gamma SNR_GAMMA\n",
            "                        SNR weighting gamma to be used if rebalancing the loss. Recommended value\n",
            "                        is 5.0. More details here: https://arxiv.org/abs/2303.09556.\n",
            "  --pre_compute_text_embeddings\n",
            "                        Whether or not to pre-compute text embeddings. If text embeddings are pre-\n",
            "                        computed, the text encoder will not be kept in memory during training and\n",
            "                        will leave more GPU memory available for training the rest of the model.\n",
            "                        This is not compatible with `--train_text_encoder`.\n",
            "  --tokenizer_max_length TOKENIZER_MAX_LENGTH\n",
            "                        The maximum length of the tokenizer. If not set, will default to the\n",
            "                        tokenizer's max length.\n",
            "  --text_encoder_use_attention_mask\n",
            "                        Whether to use attention mask for the text encoder\n",
            "  --skip_save_text_encoder\n",
            "                        Set to not save text encoder\n",
            "  --validation_images VALIDATION_IMAGES [VALIDATION_IMAGES ...]\n",
            "                        Optional set of images to use for validation. Used when the target\n",
            "                        pipeline takes an initial image as input such as when training image\n",
            "                        variation or superresolution.\n",
            "  --class_labels_conditioning CLASS_LABELS_CONDITIONING\n",
            "                        The optional `class_label` conditioning to pass to the unet, available\n",
            "                        values are `timesteps`.\n",
            "  --validation_scheduler {DPMSolverMultistepScheduler,DDPMScheduler}\n",
            "                        Select which scheduler to use for validation. DDPMScheduler is recommended\n",
            "                        for DeepFloyd IF.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AVUsMJ3gUpL3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a4a2db9-1099-4c21-9935-7469fdbf4e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "2024-03-08 22:28:31.848059: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-08 22:28:31.848125: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-08 22:28:31.849998: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-08 22:28:33.102405: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/08/2024 22:28:34 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "{'rescale_betas_zero_snr', 'dynamic_thresholding_ratio', 'variance_type', 'clip_sample_range', 'sample_max_value', 'prediction_type', 'timestep_spacing', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
            "{'latents_mean', 'latents_std', 'scaling_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "{'time_embedding_act_fn', 'num_class_embeds', 'conv_in_kernel', 'addition_embed_type', 'mid_block_type', 'only_cross_attention', 'upcast_attention', 'attention_type', 'projection_class_embeddings_input_dim', 'cross_attention_norm', 'addition_embed_type_num_heads', 'resnet_out_scale_factor', 'reverse_transformer_layers_per_block', 'class_embeddings_concat', 'dual_cross_attention', 'time_embedding_type', 'encoder_hid_dim', 'use_linear_projection', 'timestep_post_act', 'dropout', 'addition_time_embed_dim', 'num_attention_heads', 'time_embedding_dim', 'conv_out_kernel', 'transformer_layers_per_block', 'time_cond_proj_dim', 'encoder_hid_dim_type', 'resnet_skip_time_act', 'class_embed_type', 'resnet_time_scale_shift', 'mid_block_only_cross_attention'} was not found in config. Values will be initialized to default values.\n",
            "03/08/2024 22:28:58 - INFO - __main__ - ***** Running training *****\n",
            "03/08/2024 22:28:58 - INFO - __main__ -   Num examples = 3\n",
            "03/08/2024 22:28:58 - INFO - __main__ -   Num batches each epoch = 2\n",
            "03/08/2024 22:28:58 - INFO - __main__ -   Num Epochs = 200\n",
            "03/08/2024 22:28:58 - INFO - __main__ -   Instantaneous batch size per device = 2\n",
            "03/08/2024 22:28:58 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "03/08/2024 22:28:58 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "03/08/2024 22:28:58 - INFO - __main__ -   Total optimization steps = 400\n",
            "Steps: 100% 400/400 [13:38<00:00,  1.95s/it, loss=0.00534, lr=5e-6]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "{'latents_mean', 'latents_std', 'scaling_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  86% 6/7 [00:00<00:00, 37.97it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 13.06it/s]\n",
            "{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
            "Configuration saved in ./../../../drive/MyDrive/SketchToReality/dreambooth_cat_results/vae/config.json\n",
            "Model weights saved in ./../../../drive/MyDrive/SketchToReality/dreambooth_cat_results/vae/diffusion_pytorch_model.safetensors\n",
            "Configuration saved in ./../../../drive/MyDrive/SketchToReality/dreambooth_cat_results/unet/config.json\n",
            "Model weights saved in ./../../../drive/MyDrive/SketchToReality/dreambooth_cat_results/unet/diffusion_pytorch_model.safetensors\n",
            "Configuration saved in ./../../../drive/MyDrive/SketchToReality/dreambooth_cat_results/scheduler/scheduler_config.json\n",
            "Configuration saved in ./../../../drive/MyDrive/SketchToReality/dreambooth_cat_results/model_index.json\n",
            "Steps: 100% 400/400 [15:08<00:00,  2.27s/it, loss=0.00534, lr=5e-6]\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch train_dreambooth.py \\\n",
        " --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n",
        " --instance_data_dir=\"./../../../drive/MyDrive/SketchToReality/dreambooth_cat\" \\\n",
        " --instance_prompt=\"a photo of a sks cat\" \\\n",
        " --output_dir \"./../../../drive/MyDrive/SketchToReality/dreambooth_cat_results\" \\\n",
        " --use_8bit_adam \\\n",
        " --gradient_checkpointing \\\n",
        " --enable_xformers_memory_efficient_attention \\\n",
        " --set_grads_to_none \\\n",
        " --train_batch_size=2 \\\n",
        " --gradient_accumulation_steps=1 \\\n",
        " --learning_rate=5e-6 \\\n",
        " --lr_scheduler=\"constant\" \\\n",
        " --lr_warmup_steps=0 \\\n",
        " --max_train_steps=400\n",
        "\n",
        "# These parameters bring an error - I don't know why\n",
        "  # --num_train_epochs= 5 \\\n",
        "  # --checkpointing_steps= 3 \\"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PTQ8bQGWVq1a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}